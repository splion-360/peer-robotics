{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zRENlQrm7de"
   },
   "source": [
    "# CS6476 - Object Detection Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NT2lufq5m7di"
   },
   "source": [
    "# AIM\n",
    "The aim of this assignment is to implement single-stage detector - YoloV3 on PascalVOC2007 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOdh2SSTm7dj"
   },
   "source": [
    "## Learning Objectives\n",
    "- Understanding YoloV3 architecture and its components - Backbone, Feature Pyramid Network (FPN) and Yolo heads (classification and regression).\n",
    "- Developing basic understanding of post-processing concepts like Non-maxima suppression\n",
    "- Developing basic understanding of loss formulation in object detection setup\n",
    "- End-to-end training of object detection networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JK-OA6psm7dk"
   },
   "source": [
    "## Grading Schema\n",
    "The assignment will be graded out of 100.\n",
    "- Yolo head (25)\n",
    "- Yolo Forward Pass (10)\n",
    "- Yolo losses (20)\n",
    "- Non-maxima suppression (15)\n",
    "- Training Yolo and reporting mean Average Precision (mAP) (30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SER89f2Ha4Tp"
   },
   "source": [
    "## Setting Up\n",
    "Before you run this notebook on your local machine, set up a virtual environment as follows:\n",
    "\n",
    "* Install virtualenv: `pip install virtualenv`\n",
    "* Create a virtual environment: `virtualenv yolo_env -p $(which python)`. This step should create a directory in the current folder called \"yolo_env\"\n",
    "* Activate the virtual environment: `source yolo_env/bin/activate`\n",
    "* Install the required packages: `pip install -r requirements.txt`\n",
    "* Open an IPython kernel with the new environment: `ipython kernel install --user --name=yolo_env`\n",
    "* Finally open the notebook in your browser: `jupyter notebook`.\n",
    "\n",
    "You DON'T need to do the above if you are running the notebook on Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjOsh1Y5m7dl"
   },
   "source": [
    "### Google colab setup\n",
    "First, run the following cell to load the \"autoreload\" extension. The \"autoreload\" extension allows you to automatically reload (re-import) Python modules that you've imported or defined when they change. This is particularly useful when you are actively developing or modifying code in external modules and want those changes to be automatically reflected in your notebook without manually restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1695164405044,
     "user": {
      "displayName": "mousumi bose",
      "userId": "07422871141496807776"
     },
     "user_tz": 240
    },
    "id": "zrmxo2Qia4Tu"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1301,
     "status": "ok",
     "timestamp": 1695164406319,
     "user": {
      "displayName": "mousumi bose",
      "userId": "07422871141496807776"
     },
     "user_tz": 240
    },
    "id": "qsr3GxhkbkWV",
    "outputId": "1f716bb7-eb09-499c-d472-9930d6486650"
   },
   "outputs": [],
   "source": [
    "#run the below cell only on Colab\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1695164406321,
     "user": {
      "displayName": "mousumi bose",
      "userId": "07422871141496807776"
     },
     "user_tz": 240
    },
    "id": "RQK31sgQUbMa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH = \"./dataset/custom/imgs/\"\n",
    "ROOT = \"./dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXWg_zvqdP5U"
   },
   "source": [
    "Run the below cell only if you are on Colab. Change this path to the name of your extracted folder on Google Drive. For example: drive/MyDrive/my_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1695164406322,
     "user": {
      "displayName": "mousumi bose",
      "userId": "07422871141496807776"
     },
     "user_tz": 240
    },
    "id": "V8qdHuQnbJKm",
    "outputId": "cafe5776-c342-4f32-925a-016dcee7d767"
   },
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/yolov3\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "virW_x8vm7dr"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python==4.8.0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11368,
     "status": "ok",
     "timestamp": 1695164417655,
     "user": {
      "displayName": "mousumi bose",
      "userId": "07422871141496807776"
     },
     "user_tz": 240
    },
    "id": "Fl4tbBqcUf_m",
    "outputId": "79792745-8d26-4586-89a0-9f2a35791ba5"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import random\n",
    "\n",
    "from eval.evaluator import *\n",
    "import utils.gpu as gpu\n",
    "from utils.tools import *\n",
    "import config.yolov3_config_voc as cfg\n",
    "import utils.datasets as data\n",
    "from utils import cosine_lr_scheduler\n",
    "from utils.voc_yolo import parse_voc_annotation\n",
    "from model.yolov3 import Yolov3\n",
    "from model.loss.yolo_loss import YoloV3Loss\n",
    "\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1695164418444,
     "user": {
      "displayName": "mousumi bose",
      "userId": "07422871141496807776"
     },
     "user_tz": 240
    },
    "id": "eMKfiATtVDLR",
    "outputId": "94331a23-7259-49fb-ee5b-ddb4de3a0525"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU on\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CPU on\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNb232Dzm7ds"
   },
   "source": [
    "If everything is working correctly, then running the below cell should print the below filenames from assignment\n",
    "```\n",
    "['README.md', 'LICENSE', '__init__.py', 'infer_yolo_voc.py', 'train_annotation_og.txt', 'train_samples.txt', 'test_annotation_og.txt', 'test_samples.txt', 'train.py', 'test_yolo_voc.py', 'train_annotation.txt', 'test_annotation.txt', 'requirements.txt', 'model', 'eval', 'VOCtest-2007', 'VOCtrainval-2007', 'config', 'utils', 'YOLOv3.ipynb', '__pycache__', 'results', 'weight']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1695164418445,
     "user": {
      "displayName": "mousumi bose",
      "userId": "07422871141496807776"
     },
     "user_tz": 240
    },
    "id": "NFwH5z6_fHI3",
    "outputId": "f4dba610-4a65-4ba4-9d20-b7585b17d264"
   },
   "outputs": [],
   "source": [
    "YOLO_HOME = os.getcwd()\n",
    "print(os.listdir(YOLO_HOME))\n",
    "\n",
    "sys.path.append(YOLO_HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cw_aJ999m7dt"
   },
   "source": [
    "## Visualizing the VOC2007 Dataset\n",
    "\n",
    "For this assignment, you have been provided with 100 training images from the VOC2007 dataset and 25 test images. There is no need to download the dataset directly, but you can refer to the following link to understand the dataset: http://host.robots.ox.ac.uk/pascal/VOC/voc2007/\n",
    "\n",
    "Note that the original dataset has 20 data classes, but since we are working with limited compute, we will only use 10 data classes for our assignment. To see the classes we will be training and testing using please refer to: `config/yolov3_config_voc.py`.\n",
    "\n",
    "The cell below helps you to visualize 4 randomly chosen images from the training subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5547,
     "status": "ok",
     "timestamp": 1695164423973,
     "user": {
      "displayName": "mousumi bose",
      "userId": "07422871141496807776"
     },
     "user_tz": 240
    },
    "id": "zWjBp_PRa4Tz",
    "outputId": "c3f37bc1-99db-47d4-f6f1-079bab6ecc3e"
   },
   "outputs": [],
   "source": [
    "from train import Trainer\n",
    "\n",
    "dummy_dataset = data.VocDataset(anno_file_type=\"train\", img_size=cfg.TRAIN[\"TRAIN_IMG_SIZE\"], vis=True)\n",
    "dataloader = DataLoader(dummy_dataset, shuffle=True, batch_size=1, num_workers=0)\n",
    "\n",
    "for i, (img, label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes) in enumerate(dataloader):\n",
    "    if i<4:\n",
    "        if (img.shape[0]==1):\n",
    "            labels = np.concatenate([label_sbbox.reshape(-1, 16), label_mbbox.reshape(-1, 16),\n",
    "                                     label_lbbox.reshape(-1, 16)], axis=0)\n",
    "            labels_mask = labels[..., 4]>0\n",
    "            labels = np.concatenate([labels[labels_mask][..., :4], np.argmax(labels[labels_mask][..., 6:],\n",
    "                                    axis=-1).reshape(-1, 1)], axis=-1)\n",
    "            tools.plot_box(labels, img, id=1)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1s_S7XHm7dt"
   },
   "source": [
    "# YoloV3 Architecture\n",
    "\n",
    "Comprises of primarily 3 modules:\n",
    "- Darknet53 backbone\n",
    "- Feature Pyramid Network (FPN)\n",
    "- Yolo heads (classification and regression)\n",
    "\n",
    "Refer to original paper (https://pjreddie.com/media/files/papers/YOLOv3.pdf) for more details\n",
    "\n",
    "We have already provided the backbone and FPN's architecture in the code. You need to complete\n",
    "- Writing Yolo heads\n",
    "- initializing both FPN and Yolo heads with correct parameters\n",
    "- Completing the YoloV3 forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWXZhCEZm7du"
   },
   "source": [
    "## Feature Pyramid Network (FPN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maT19zKIm7du"
   },
   "source": [
    "## Yolo Heads\n",
    "\n",
    "Here, you'll implement the logic for converting FPN predictions -> box coordinates ```(dx,dy,dw,dh)```, box confidence and class probablities (both in range ```[-1,1]```) into     final box coordinates ```(x,y,w,h)```, box confidence and class probabilities (both in range ```[0,1]```)\n",
    "\n",
    "Head over to **TODO** for `forward` function of `Yolo_head` class. Once you complete that, run the following to check your implementation\n",
    "\n",
    "**Note:** The autograder will have more comprehensive test case where it will check the predicted values from `Yolo_head` (and not just the range).\n",
    "\n",
    "**GradeScope Submission:**\n",
    "- Upload `yolo_head.py` to gradescope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "## Calculating the x and y coordinates from the cell offsets\n",
    "dx,dy = p[...,0],p[...,1]\n",
    "cy,cx = torch.arange(end=dx.shape[1]).repeat(dx.shape[1],1), torch.arange(end=dx.shape[1]).repeat(dx.shape[1],1).T       \n",
    "cx = cx[...,None].repeat(1,1,3)\n",
    "cy = cy[...,None].repeat(1,1,3)\n",
    "x,y = torch.sigmoid(dx)+cx, torch.sigmoid(dy)+cy\n",
    "\n",
    "## Calculating the width and height of the box using the dw,dh. Scaling of anchor box is required to remain consistent with the local \n",
    "## calculation\n",
    "dw,dh = p[...,2],p[...,3]\n",
    "pW,pH = (anchors/stride)[:,0],(anchors/stride)[:,1]\n",
    "pW,pH = pW[None,None,None,...],pH[None,None,None,...]\n",
    "w,h = torch.exp(dw)*pW, torch.exp(dh)*pH\n",
    "\n",
    "## Mapping the box and class probabilities between (0,1) \n",
    "p[:,:,:,:,4:] = torch.sigmoid(p[:,:,:,:,4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1695164423975,
     "user": {
      "displayName": "mousumi bose",
      "userId": "07422871141496807776"
     },
     "user_tz": 240
    },
    "id": "ZQehOH8Km7du",
    "outputId": "873944cd-a52b-45e1-9ee4-af3099db190f"
   },
   "outputs": [],
   "source": [
    "from model.head.yolo_head import Yolo_head\n",
    "\n",
    "##### Test Output Shapes #####\n",
    "nC = 20 # For example, let's assume we have 20 classes\n",
    "anchors = torch.Tensor([[10,13], [16,30], [33,23]]) # Sample anchors\n",
    "stride = 32\n",
    "batch_size = 8\n",
    "grid_size = 13 # Grid size\n",
    "\n",
    "# Instantiate Yolo_head object\n",
    "yolo_head = Yolo_head(nC, anchors, stride)\n",
    "\n",
    "# Simulate the input tensor\n",
    "p = torch.randn(batch_size, 3 * (5 + nC), grid_size, grid_size)\n",
    "\n",
    "# Call the forward method\n",
    "p, p_d = yolo_head(p)\n",
    "\n",
    "# Check shape of the outputs\n",
    "assert p.shape == (batch_size, grid_size, grid_size, 3, 5 + nC)\n",
    "assert p_d.shape == (batch_size, grid_size, grid_size, 3, 5 + nC)\n",
    "print('Test Case 1 Passed!')\n",
    "\n",
    "\n",
    "\n",
    "##### Test Output range values #####\n",
    "nC = 20\n",
    "anchors = torch.Tensor([[10,13], [16,30], [33,23]])\n",
    "stride = 32\n",
    "batch_size = 8\n",
    "grid_size = 13\n",
    "\n",
    "yolo_head = Yolo_head(nC, anchors, stride)\n",
    "\n",
    "p = torch.randn(batch_size, 3 * (5 + nC), grid_size, grid_size)\n",
    "_, p_d = yolo_head(p)\n",
    "\n",
    "# Extract components from the decoded tensor\n",
    "xy = p_d[..., 0:2]\n",
    "wh = p_d[..., 2:4]\n",
    "conf = p_d[..., 4:5]\n",
    "prob = p_d[..., 5:]\n",
    "\n",
    "# Check xy range\n",
    "assert (xy >= 0).all() and (xy <= grid_size * stride).all(), \"xy values are out of valid range!\"\n",
    "# Check wh positivity\n",
    "assert (wh > 0).all(), \"w,h values should be positive!\"\n",
    "# Check confidence score range\n",
    "assert (conf >= 0).all() and (conf <= 1).all(), \"Confidence scores are out of [0,1] range!\"\n",
    "# Check class probabilities range\n",
    "assert (prob >= 0).all() and (prob <= 1).all(), \"Class probabilities are out of [0,1] range!\"\n",
    "print('Test Case 2 Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yMlM19am7dv"
   },
   "source": [
    "## YoloV3 Forward Pass\n",
    "Now, as all 3 modules are there, head over to **TODO** section of `yolov3.py`. Once you complete that, run the following to check your implementation.\n",
    "\n",
    "**GradeScope Submission:**\n",
    "- Change the following import\n",
    "`\n",
    "from model.head.yolo_head import Yolo_head\n",
    "`\n",
    "to\n",
    "`\n",
    "from student_submission import Yolo_head\n",
    "`\n",
    "before submission.\n",
    "- Upload `yolov3.py` to gradescope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 696,
     "status": "ok",
     "timestamp": 1695164424660,
     "user": {
      "displayName": "mousumi bose",
      "userId": "07422871141496807776"
     },
     "user_tz": 240
    },
    "id": "_XzC3U3bm7dv",
    "outputId": "01a79c65-121d-40cc-e5bb-74b53ea5bc83"
   },
   "outputs": [],
   "source": [
    "from model.yolov3 import Yolov3\n",
    "\n",
    "## Testing FPN output shapes\n",
    "model = Yolov3(cfg)\n",
    "batch_size = 1\n",
    "channels = 3\n",
    "input_size = 448\n",
    "num_classes = 10\n",
    "anchors = torch.Tensor([[10,13], [16,30], [33,23]])\n",
    "num_anchors_per_scale = anchors.shape[0]\n",
    "input_img = torch.randn(batch_size, channels, input_size, input_size)\n",
    "\n",
    "x_s, x_m, x_l = model._Yolov3__backnone(input_img)\n",
    "\n",
    "x_s, x_m, x_l = model._Yolov3__fpn(x_l, x_m, x_s)\n",
    "C = num_anchors_per_scale*(num_classes + 5)\n",
    "assert x_s.shape == (batch_size, C, 56, 56), \"FPN output x_s shape {} does not match with expected shape {}\".format(x_s.shape, (batch_size, C, 56, 56))\n",
    "assert x_m.shape == (batch_size, C, 28, 28), \"FPN output x_m shape {} does not match with expected shape {}\".format(x_m.shape, (batch_size, C, 28, 28))\n",
    "assert x_l.shape == (batch_size, C, 14, 14), \"FPN output x_l shape {} does not match with expected shape {}\".format(x_l.shape, (batch_size, C, 14, 14))\n",
    "print('FPN output shapes test passed!!')\n",
    "\n",
    "## Testing YoloV3 forward pass shape\n",
    "channels = 3\n",
    "input_size = 448\n",
    "num_classes = 10\n",
    "anchors = torch.Tensor([[10,13], [16,30], [33,23]])\n",
    "num_anchors_per_scale = anchors.shape[0]\n",
    "input_img = torch.randn(batch_size, channels, input_size, input_size)\n",
    "\n",
    "yolov3_output = model(input_img)\n",
    "\n",
    "C = num_anchors_per_scale*(num_classes + 5)\n",
    "\n",
    "assert type(yolov3_output) is tuple, \"YoloV3 forward output should be tuple containing p and p_d. Go through the docstring for more details\"\n",
    "assert len(yolov3_output) == 2, \"YoloV3 output should be a tuple of length 2. But got {} length instead\".format(len(yolov3_output))\n",
    "p, p_d = yolov3_output\n",
    "assert p[0].shape == (batch_size, 56, 56, channels, num_classes+5), \"p[0] shape {} does not match with expected shape {}\".format(p[0].shape, (batch_size, 56, 56, channels, num_classes+5))\n",
    "assert p[1].shape == (batch_size, 28, 28, channels, num_classes+5), \"p[1] shape {} does not match with expected shape {}\".format(p[1].shape, (batch_size, 28, 28, channels, num_classes+5))\n",
    "assert p[2].shape == (batch_size, 14, 14, channels, num_classes+5), \"p[2] shape {} does not match with expected shape {}\".format(p[2].shape, (batch_size, 14, 14, channels, num_classes+5))\n",
    "assert p_d[0].shape == (batch_size, 56, 56, channels, num_classes+5), \"p_d[0] shape {} does not match with expected shape {}\".format(p_d[0].shape, (batch_size, 56, 56, channels, num_classes+5))\n",
    "assert p_d[1].shape == (batch_size, 28, 28, channels, num_classes+5), \"p_d[1] shape {} does not match with expected shape {}\".format(p_d[1].shape, (batch_size, 28, 28, channels, num_classes+5))\n",
    "assert p_d[2].shape == (batch_size, 14, 14, channels, num_classes+5), \"p_d[2] shape {} does not match with expected shape {}\".format(p_d[2].shape, (batch_size, 14, 14, channels, num_classes+5))\n",
    "print('YoloV3 Forward pass output shapes test passed!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVLHU-r6m7dv"
   },
   "source": [
    "## YoloV3 loss optimization\n",
    "\n",
    "Now, the forward pass of YoloV3 is completed. YoloV3 is trained using classification, regression and confidence losses. Confidence loss is already provided.\n",
    "\n",
    "Head over to `TODO` section of `yolo_loss.py`, and complete the regression and confidence losses. Once you complete that, run the following to check your implementation.\n",
    "\n",
    "**NOTE**: The below cell only checks type and output range of loss. Please check the correctness of your implementation in gradescope\n",
    "\n",
    "**GradeScope Submission:**\n",
    "- Upload `yolo_loss.py` to gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3747,
     "status": "ok",
     "timestamp": 1695164428396,
     "user": {
      "displayName": "mousumi bose",
      "userId": "07422871141496807776"
     },
     "user_tz": 240
    },
    "id": "TzMxuu6Gm7dw",
    "outputId": "03cf2cf9-6cc5-42c5-fb4b-73a954102928"
   },
   "outputs": [],
   "source": [
    "from model.loss.yolo_loss import YoloV3Loss\n",
    "from model.yolov3 import Yolov3\n",
    "\n",
    "yolov3 = Yolov3(cfg)\n",
    "loss_fn = YoloV3Loss(cfg.MODEL[\"ANCHORS\"], cfg.MODEL[\"STRIDES\"])\n",
    "\n",
    "p, p_d = yolov3(torch.rand(1, 3, 448, 448))\n",
    "label_sbbox = torch.rand(1, 56, 56, 3, 16)\n",
    "label_mbbox = torch.rand(1, 28, 28, 3, 16)\n",
    "label_lbbox = torch.rand(1, 14, 14, 3, 16)\n",
    "sbboxes = torch.rand(1, 150, 4)\n",
    "mbboxes = torch.rand(1, 150, 4)\n",
    "lbboxes = torch.rand(1, 150, 4)\n",
    "\n",
    "loss, loss_giou, loss_conf, loss_cls = loss_fn(p, p_d, label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes)\n",
    "\n",
    "assert isinstance(loss, torch.Tensor), \"loss Tensor of type {}, should be torch.Tensor instead\".format(type(loss))\n",
    "assert isinstance(loss_giou, torch.Tensor), \"loss_giou Tensor of type {}, should be torch.Tensor instead\".format(type(loss_giou))\n",
    "assert isinstance(loss_conf, torch.Tensor), \"loss_conf Tensor of type {}, should be torch.Tensor instead\".format(type(loss_conf))\n",
    "assert isinstance(loss_cls, torch.Tensor), \"loss_cls Tensor of type {}, should be torch.Tensor instead\".format(type(loss_cls))\n",
    "\n",
    "assert loss>=0, \"loss Tensor should be >=0\"\n",
    "assert loss_giou>=0, \"loss_giou Tensor should be >=0\"\n",
    "assert loss_conf>=0, \"loss_conf Tensor should be >=0\"\n",
    "assert loss_cls>=0, \"loss_cls Tensor should be >=0\"\n",
    "print('Loss type and range value test - Passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbboxes = torch.rand(1, 150, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sbboxes[...,2]*sbboxes[...,3]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCWW7vZYm7dw"
   },
   "source": [
    "## Post-processing: Non-maxima Suppression\n",
    "\n",
    "Outputs from object detectors generally tend to have multiple redundant predictions per object. Hence, we'll implement an algorithm - Non-maxima suppression to suppress the redundant predictions and only keep the most confident one having highest IoU with other corresponding predictions.\n",
    "\n",
    "Head over to `TODO` section of `tools.py` and complete `nms()`. Once you complete that, run the following to check your implementation.\n",
    "\n",
    "**Gradescope Submission:**\n",
    "- Upload `utils\\tools.py` file to gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1695164428397,
     "user": {
      "displayName": "mousumi bose",
      "userId": "07422871141496807776"
     },
     "user_tz": 240
    },
    "id": "xwQbZLWHm7dw",
    "outputId": "15f9ca3b-9582-449c-84c1-8fef299690a7"
   },
   "outputs": [],
   "source": [
    "from utils.tools import nms\n",
    "\n",
    "## Test NMS Basic Functionality\n",
    "bboxes = np.array([[10, 10, 50, 50, 0.9, 1], [15, 15, 55, 55, 0.8, 1]])\n",
    "gt_bboxes = np.array([[10., 10., 50., 50., 0.9, 1.]])\n",
    "score_threshold = 0.7\n",
    "iou_threshold = 0.5\n",
    "output_bboxes = nms(bboxes, score_threshold, iou_threshold)\n",
    "assert np.allclose(np.sort(output_bboxes.copy().flatten()), np.sort(gt_bboxes.copy().flatten())), \"Output boxes {}  and ground truth boxes {} do not match \".format(output_bboxes, gt_bboxes)\n",
    "print('NMS Basic Functionality Test - Passed')\n",
    "\n",
    "## Bounding box of multiple classes overlap\n",
    "bboxes= np.array([[10, 10, 50, 50, 0.9, 1], [15, 15, 55, 55, 0.8, 1], [100, 100, 150, 150, 0.95, 2], [105, 105, 155, 155, 0.85, 2]])\n",
    "score_threshold = 0.7\n",
    "iou_threshold = 0.5\n",
    "gt_bboxes = np.array([[10, 10, 50, 50, 0.9, 1], [100, 100, 150, 150, 0.95, 2]])\n",
    "output_bboxes = nms(bboxes, score_threshold, iou_threshold)\n",
    "assert gt_bboxes.shape[0] == output_bboxes.shape[0], \"No. of GT boxes {} and output boxes {} are not equal\".format(gt_bboxes.shape[0], output_bboxes.shape[0])\n",
    "assert np.allclose(np.sort(output_bboxes.copy().flatten()), np.sort(gt_bboxes.copy().flatten())), \"Output boxes {}  and ground truth boxes {} do not match \".format(output_bboxes, gt_bboxes)\n",
    "print('NMS - Multiple class bounding box overlap - Passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUegvz3Om7dx"
   },
   "source": [
    "## Training YoloV3\n",
    "Now, train the YoloV3 architecture end-to-end using the above Yolo losses, and report the **final mAP**.\n",
    "\n",
    "**Note:** The best model gets saved with name `best.pt` under the directory `weight`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EWlyWsvm7dx"
   },
   "source": [
    "### Creating a Results Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1695164428398,
     "user": {
      "displayName": "mousumi bose",
      "userId": "07422871141496807776"
     },
     "user_tz": 240
    },
    "id": "Ebd0dZ5la4T0"
   },
   "outputs": [],
   "source": [
    "if (not os.path.exists('results/voc')):\n",
    "    os.mkdir(os.path.join(YOLO_HOME, 'results'))\n",
    "    os.mkdir(os.path.join(YOLO_HOME, 'results/voc'))\n",
    "\n",
    "    for label in cfg.DATA['CLASSES']:\n",
    "        filename = 'comp4_det_test_'+label+'.txt'\n",
    "        os.system('touch results/voc/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1307883,
     "status": "ok",
     "timestamp": 1695165736267,
     "user": {
      "displayName": "mousumi bose",
      "userId": "07422871141496807776"
     },
     "user_tz": 240
    },
    "id": "OAePua0Qa4T1",
    "outputId": "415a8925-f243-4245-a806-827efc152d64"
   },
   "outputs": [],
   "source": [
    "Trainer(weight_path='weight/darknet53_448.weights',\n",
    "            resume=False,\n",
    "            gpu_id=0).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
